---
title: 'Chapter 6: Linear Model & Regularization'
output:
  html_document:
    df_print: paged
---


=========================
**Subset Selection Model**
=========================

Loading the required packages
```{r}
library(ISLR)
require(leaps)
require(dplyr)
```

Loading the dataset
```{r}
data("Hitters")
summary(Hitters)
```

There are few NAs in Salary column
We will omit all records with missing values

```{r}
Hitters <- na.omit(Hitters)
colSums(is.na(Hitters))
```

*Best Subset Regression*

```{r}
bestsub.model <- regsubsets(Salary ~ ., data = Hitters)
summary(bestsub.model)
```

This gives us best model out of all the models having same number of predictors.
By default, the above function ran for models with 8 or less predictors. But we have a total of 19 predictor variables. So overwriting this default behavior to get models upto 19 in size

```{r}
bestsub.model <- regsubsets(Salary ~ ., data = Hitters, nvmax = 19)
(bestsub.summary <- summary(bestsub.model))
```

Now we need to select the model (and therefore the optimal number of predictors) that fits the data well from the above generated models

We will see the Cp criterion to determine the best model
```{r}
plot(bestsub.summary$cp, 
     xlab = "Number of predictors",
     ylab = "Cp")
```

```{r}
which.min(bestsub.summary$cp)
```

Model with 10 predictors is the one that has the lowest Cp 

```{r}
plot(bestsub.model, scale = "Cp")
```

```{r}
coef(bestsub.model,10)
```

***

**Forward Stepwise Regression**

We use the same method 'regsubsets' with method = "forward"
```{r}
model.fwd <- regsubsets(Salary ~ . , 
                        data = Hitters, 
                        method = "forward",
                        nvmax = 19)
(model.fwd.summary <- summary(model.fwd))
```

```{r}
plot(model.fwd, scale = "Cp")
```

Selecting the best model

```{r}
which.min(model.fwd.summary$cp)
```

Forward selection also gives the model with 10 predictor variables as the optimal

**Model Selection Using Validation Set**

```{r}
set.seed(1)

#Dividing the dataset into Train and Validation (70:30)
n <- nrow(Hitters)
train_idx <- sample(seq(n),180, replace = F)
```

Fit model using the training dataset
```{r}
train.model.fwd <- regsubsets(Salary ~ ., 
                              data = Hitters[train_idx,],
                              nvmax = 19,
                              method = "forward")

(train.fwd.summary <- summary(train.model.fwd))
```

Now we will fit each of these 19 models on our Validation set to compare how they perform on new data

```{r}
val.err <- rep(NA, 19)

test.matrix <- model.matrix(Salary ~., data = Hitters[-train_idx,])

for(i in 1:19)
{
  #Extracting the coefficients from the model with i predictors
  coeff <- coef(train.model.fwd, id = i)
  
  #Matrix multiplication of the test dataset values and coefficients
  val.pred <- test.matrix[,names(coeff)] %*% coeff
  
  
  #Mean Sum of Squared Errors
  val.err[i] <- mean((Hitters$Salary[-train_idx] - val.pred)^2)
}

plot(sqrt(val.err),
     ylab = "Root MSE",
     type = "b",
     ylim = c(300,400),
     pch = 19)

points(sqrt(train.model.fwd$rss[-1]/180), col = "blue", pch = 19, type = "b")
```


=========================
**Model Selection using Cross Validation**
=========================

Creating a Predict function to accept regsubsets object
```{r}
predict.regsubsets <- function(regobj, newdata, id, ...)
{
  form <- as.formula(regobj$call[[2]])
  mat <- model.matrix(form, newdata)
  coeff <- coef(regobj, id = id)
  mat[,names(coeff)] %*% coeff
}
```


----------------------
Using 10-fold cross validation

```{r}
set.seed(11)

#Assign each observation to a random fold
folds <- sample(rep(1:10, length = nrow(Hitters)))
folds

table(folds)

#Matrix to hold prediction error when model of different sizes(1...19) is fit on the observations belonging to each fold
cv.err <- matrix(NA, 10, 19)

for(k in 1:10)
{
  #Fit the regression model of different length to all obs except those in the kth fold
  fit.model <- regsubsets(Salary ~., 
                          data = Hitters[folds!=k,],
                          nvmax = 19,
                          method = "forward")
  
  #In each fold, for each model predict for the observations in the kth fold and record the prediction error
  for(i in 1:19)
  {
    pred <- predict.regsubsets(fit.model, Hitters[folds == k,], id = i)
    cv.err[k,i] = mean((Hitters$Salary[folds == k] - pred)^2)
  }
}


```

```{r}
#Average out the prediction error for models of different lengths and then calculate RMSE value for each model
cv.rmse <- sqrt(apply(cv.err, 2, mean))  

plot(cv.rmse, pch = 19, type = 'b')
```
Model of size 10 is favored



***

=========================
**Exercise**
=========================

Q9. **College dataset** - Predict the number of applications received using other variables


Loading the dataset
```{r}
data(College)
glimpse(College)
```


(a) Split the data into training set and test set

```{r}
##Dividing the dataset into train and test set in 70:30 ratio

train_idx <- sample(nrow(College), size = 0.7*nrow(College))
college.train <- College[train_idx,]
college.test <- College[-train_idx,]

```

(b) Fit a linear model using least squares on the training set, and
report the test error obtained.

Null model
```{r}
lm.model0 <- lm(Apps ~ 1, data = College) 
```

Full model
```{r}
lm.model.full <- lm(Apps~., data = College)
```

Step AIC forward
```{r}
model.aic.fwd <- step(lm.model0, 
                      scope = list(lower = lm.model0, upper = lm.model.full),
                      direction = "forward",
                      trace = 0)

summary(model.aic.fwd)
```


(c) Fit a ridge regression model on the training set, with ?? chosen
by cross-validation. Report the test error obtained.

```{r}

```



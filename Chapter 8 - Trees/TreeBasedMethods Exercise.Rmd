---
title: "ISLR - Chapter 8 (Tree Based Methods)"
author: "Meenal Narsinghani"
subtitle: Exercise (Applied)
output:
  html_document:
    df_print: paged
---

7. In the lab, we applied random forests to the Boston data using mtry=6 and using ntree=25 and ntree=500. Create a plot displaying the test error resulting from random forests on this data set for a more comprehensive range of values for mtry and ntree. You can model your plot after Figure 8.10. Describe the results obtained.

```{r}
require(MASS)
require(randomForest)
```

```{r}
data("Boston")
dim(Boston)  
```
*We have a total of 13 predictor variables.*

Splitting the data into train and test
```{r}
set.seed(123)
train_idx <- sample(1:nrow(Boston), 300)
```

Fitting the random forest by building different number of trees - 1, 100, 200,..500
For each of these iterations let us try differnt values of mtry - p, p/2, sqrt(p)

```{r}
ntrees <- c(1, 100, 200, 300, 400, 500)
mtry <- c(13, 6, 4)
test.err <- matrix(data = NA, nrow = 3, ncol = 6)
test.err[1,2] <- 2

i<- 1

for(p in mtry)
{
  j <- 1  
  for(trees in ntrees)
  {
    rf.fit <- randomForest(medv ~., data = Boston[train_idx,],
                           ntree = trees,
                           mtry = p)
  
    test.pred <- predict(rf.fit, newdata = Boston[-train_idx,])
  
    test.err[i,j] <- mean((test.pred - Boston$medv[-train_idx])^2)
    j <- j + 1
  }
  i <- i + 1    
}

test.err <- as.data.frame(test.err) 
```

```{r}

test.err.df = t(test.err)

matplot(x = c(1,100,200,300,400,500),
        y = cbind(test.err.df[,1], test.err.df[,2], test.err.df[,3]), 
        pch = 19, col = c("red", "blue", "green"), type = "b", 
        xlab = "Number of trees", ylab = "MSE")
legend("topright", legend = c("mtry = 13", "mtry = 6", "mtry = 4"), pch = 19, col = c("red", "blue", "green"))
```

Fitting random forest using differet values for mtry, we notice that when mtry = 4 [~ sqrt(13)], the estimate for test error is lower than when mtry = 13 or 13/2 irrespective of the number of fitted trees.

For all the values of mtry, the test error estimate levels at about 200 trees.


***

8. In the lab, a classification tree was applied to the Carseats data set after converting Sales into a qualitative response variable. Now we will seek to predict Sales using regression trees and related approaches, treating the response as a quantitative variable.

```{r}
data("Carseats")
dim(Carseats)
```

(a) Split the data set into a training set and a test set.
```{r}
set.seed(100)
train_idx <- sample(1:nrow(Carseats), 0.5*nrow(Carseats))
train_data <- Carseats[train_idx, ]
test_data <- Carseats[-train_idx, ]
```


(b) Fit a regression tree to the training set. Plot the tree, and interpret the results. What test MSE do you obtain?
```{r}
tree_fit <- tree(Sales ~., data = train_data)

plot(tree_fit)
text(tree_fit, pretty = 0,cex = 0.6)

summary(tree_fit)
```

```{r}
pred_test <- predict(tree_fit, newdata = test_data)
test_mse <- mean((test_data$Sales - pred_test)^2)
test_mse
```

(c) Use cross-validation in order to determine the optimal level of tree complexity. Does pruning the tree improve the test MSE?
```{r}
cv_tree <- cv.tree(tree_fit, FUN = prune.tree) 
cv_tree 
```
```{r}
plot(cv_tree)
```

```{r}
cv_result <- as.data.frame(cbind(cv_tree$size, cv_tree$dev))
colnames(cv_result) <- c("size", "dev")
tree_size <- cv_result$size[cv_result$dev == min(cv_result$dev)]
```

```{r}
prune_tree <- prune.tree(tree_fit, best = tree_size[2])  #best = #of terminal nodes
plot(prune_tree); text(prune_tree, pretty = 0, cex = 0.6)
```
```{r}
prune_pred <- predict(prune_tree, newdata = test_data)
prune_test_mse <- mean((test_data$Sales - prune_pred)^2)
prune_test_mse

```


(d) Use the bagging approach in order to analyze this data. What test MSE do you obtain? Use the importance() function to determine which variables are most important.
```{r}

```


(e) Use random forests to analyze this data. What test MSE do you obtain? Use the importance() function to determine which variables are most important. Describe the effect of m, the number of variables considered at each split, on the error rate obtained.
